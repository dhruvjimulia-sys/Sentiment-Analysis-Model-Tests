{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "###### Import Libraries ######\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\r\n",
        "\r\n",
        "import spacy\r\n",
        "nlp = spacy.load(\"en_core_web_md\")\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "# from tensorflow import sparse\r\n",
        "from tensorflow import keras \r\n",
        "\r\n",
        "from scipy import sparse\r\n",
        "\r\n",
        "print(\"Imports complete\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "####### Data Preprocessing Functions #######\n",
        "def is_not_br(token):\n",
        "    return ((not ('<br' in token.text)) & (not ('/><br' in token.text)) & (not ('/>' in token.text)) & (token.text != 'br') & (not ('<' in token.text)))\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct) & is_not_br(token) & (not token.text.isdigit()):\n",
        "            clean_tokens.append(token.lemma_.lower())\n",
        "    return clean_tokens\n",
        "# Maps 'positive' to 1 and 'negative' to 0\n",
        "def convert_y(sentiments):\n",
        "    converted_list = []\n",
        "    for sentiment in sentiments:\n",
        "        converted_list.append(1 if sentiment == 'positive' else 0)\n",
        "    return np.array(converted_list)\n",
        "# Normalization function before neural network: normalizes all values between 0 and 1\n",
        "def normalize(arr):\n",
        "    max_val = np.max(arr)\n",
        "    min_val = np.min(arr)\n",
        "    # normalized_array = np.array([2 * ((val - min_val)/(max_val - min_val)) for val in arr])\n",
        "    normalized_array = 2 * ((arr - min_val)/(max_val - min_val)) - 1\n",
        "    return normalized_array\n",
        "\n",
        "#%%\n",
        "###### Loading Data ######\n",
        "imdb = pd.read_csv('../imdbDataset.csv')\n",
        "imdb = imdb.iloc[:1500]\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "print(f\"Length : {len(imdb)}\")\n",
        "\n",
        "print(\"Data Loading Complete\")\n",
        "\n",
        "#%%\n",
        "##### Word -> Number Encoding ######\n",
        "##### Bag Of Words\n",
        "##### Hyperparams: max_features\n",
        "def bag_of_words(X, y):\n",
        "    print(\"Starting Bag of Words Model\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"r\") as fin:\n",
        "        if ('False' in fin.read()):\n",
        "            print(\"Did not find vectorizer. Creating new vectorizer...\")\n",
        "            bow_transformer = CountVectorizer(analyzer=tokenize, max_features=2000).fit(X)\n",
        "            X = bow_transformer.transform(X)\n",
        "            pickle.dump(X, open(\"./preencoded_embeddings/bow_transformer_vectors.pickle\", \"wb\"))\n",
        "        else:\n",
        "            print(\"Vectorizer Found. Loading Vectorizer...\")\n",
        "            X = pickle.load(open(\"./preencoded_embeddings/bow_transformer_vectors.pickle\", \"rb\"))\n",
        "        y = convert_y(y)\n",
        "        print(\"Bag of Words Model Completed\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"w\") as fout:\n",
        "        fout.write(\"True\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### TF-IDF Vectorization\n",
        "##### Hyperparams: max_features\n",
        "def tf_idf(X, y):\n",
        "    print(\"Starting TF-IDF Model\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"r\") as fin:\n",
        "        if ('False' in fin.read()):\n",
        "            print(\"Did not find vectorizer. Creating new vectorizer...\")\n",
        "            tfidf_transformer = TfidfVectorizer(analyzer=tokenize, max_features=2000).fit(X)\n",
        "            X = tfidf_transformer.transform(X)\n",
        "            pickle.dump(X, open(\"./preencoded_embeddings/tfidf_vectors.pickle\", \"wb\"))\n",
        "        else:\n",
        "            print(\"Vectorizer Found. Loading Vectorizer...\")\n",
        "            X = pickle.load(open(\"./preencoded_embeddings/tfidf_vectors.pickle\", \"rb\"))\n",
        "        y = convert_y(y)\n",
        "        print(\"TF-IDF Model Created\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"w\") as fout:\n",
        "        fout.write(\"True\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### Pre-trained Word Embeddings (Word2Vec Twitter Model)\n",
        "##### Hyperparams: \n",
        "def word2vec(X, y):\n",
        "    print(\"Starting Word2Vec Model\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"r\") as fin:\n",
        "        if ('False' in fin.read()):\n",
        "            print(\"Using pretrained Spacy vectors to train model\")\n",
        "            print(\"No reviews found. Converting reviews to vectors...\")\n",
        "            vectorized_reviews = []\n",
        "            for index, review in enumerate(X):\n",
        "                avg_vector = np.zeros(shape=(300))\n",
        "                for token in tokenize(review):\n",
        "                    avg_vector += nlp(token).vector\n",
        "                avg_vector = avg_vector / len(X)\n",
        "                vectorized_reviews.append(avg_vector)\n",
        "                print(f\"Review {index}: Done\")\n",
        "            pickle.dump(vectorized_reviews, open(\"./preencoded_embeddings/word2vec_reviews.pickle\", \"wb\"))\n",
        "        else:\n",
        "            print(\"Vectorized reviews found. Loading word vectors...\")\n",
        "            vectorized_reviews = pickle.load(open(\"./preencoded_embeddings/word2vec_reviews.pickle\", \"rb\"))\n",
        "    y = convert_y(y)    \n",
        "    print(\"Word2Vec Model Created\")\n",
        "    with open(\"./check_data_present/is_vectorizer_created\", \"w\") as fout:\n",
        "        fout.write(\"True\")\n",
        "    return normalize(np.array(vectorized_reviews)), y\n",
        "\n",
        "##### BERT Language Model\n",
        "\n",
        "##### Model Types ######\n",
        "##### Logistic Regression\n",
        "##### Hyperparams: train_test_split, regularization_type, C (inverse of regularization strength), \n",
        "def logistic_regression(X, y):\n",
        "    print(\"Starting creation of Logistic Regression Model\")\n",
        "    logistic_model = LogisticRegression()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "    y_pred = logistic_model.predict(X_test)\n",
        "    print(\"Logistic Regression Model created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### K-Nearest-Neighbours Classifier\n",
        "##### Hyperparams: n_neighbours, train_test_split\n",
        "def knn_classifier(X, y, number_neighbors):\n",
        "    print(\"Started creation of KNN Classifier\")\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=number_neighbors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    y_pred = knn_classifier.predict(X_test)\n",
        "    print(\"KNN Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Naive Bayes Classifier\n",
        "##### Hyperparams: train_test_split\n",
        "def naive_bayes(X, y):\n",
        "    print(\"Started creation of Naive Bayes Classifier\")\n",
        "    nb_model = MultinomialNB()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    print(\"Naive Bayes Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Neural Network\n",
        "##### Hyperparams: train_test_split, Architecture, Optimizer: Learning Rate, Epochs, Batch Size, Initial Weights, Initial Biases\n",
        "#####              Epochs, Loss Function, Regularization\n",
        "def neural_network(X, y, architecture_id):\n",
        "    print(\"Started creation of Neural Network\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n",
        "    if architecture_id == 1:\n",
        "        model = keras.Sequential([\n",
        "                keras.layers.Dense(300, activation=\"tanh\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(150, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=64)\n",
        "    elif architecture_id == 2:\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(2000, activation=\"relu\"),\n",
        "            keras.layers.Dense(200, activation=\"relu\"),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "    \n",
        "    model.summary()\n",
        "    y_pred = model.predict(X_test)\n",
        "    argmax_predictions = np.array([round(array[0]) for array in y_pred])\n",
        "    print(\"Neural Network Created\")\n",
        "    return y_test, argmax_predictions"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "##### Applying Models And Printing Accuracy #####\n",
        "\n",
        "X, y = word2vec(imdb['review'], imdb['sentiment'])\n",
        "y_test, y_pred = neural_network(X, y, 1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall: \", round(recall, 2))\n",
        "\n",
        "##### KNN Classifier Implementation\n",
        "# max_accuracy = -1\n",
        "# max_accuracy_neighbors = 0\n",
        "\n",
        "# for i in range(1, 10):\n",
        "#     print(f\"Number of neighbors: {i}\")\n",
        "#     y_test, y_pred = knn_classifier(X, y, i)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred)\n",
        "#     recall = recall_score(y_test, y_pred)\n",
        "\n",
        "#     print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall \", round(recall, 2))\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#     if (accuracy > max_accuracy):\n",
        "#         max_accuracy = accuracy\n",
        "#         max_accuracy_neighbors = i\n",
        "\n",
        "# print(f\"Maximum Accuracy obtained was {max_accuracy} with {max_accuracy_neighbors}\")"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}