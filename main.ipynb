{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB89UKxnL2nz"
      },
      "source": [
        "##### Global Variables ######\n",
        "create_new_vectors = False\n",
        "data_size = 1500"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8-2-YM7L2n3",
        "outputId": "d551621c-6565-409f-db0c-03969aa25221"
      },
      "source": [
        "###### Import Libraries ######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow import keras \n",
        "from scipy import sparse\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Imports complete\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051304 sha256=5252ac2a016a8b671ae3063b5e4d28bb15fa188791515d04d65a26d8ad9621cc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dp0d_mrt/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Imports complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drraVU-VL2n4",
        "outputId": "91729c0f-1545-40d5-fa1a-c0616db287cf"
      },
      "source": [
        "####### Data Preprocessing Functions #######\n",
        "def is_not_br(token):\n",
        "    return ((not ('<br' in token.text)) & (not ('/><br' in token.text)) & (not ('/>' in token.text)) & (token.text != 'br') & (not ('<' in token.text)))\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct) & is_not_br(token) & (not token.text.isdigit()):\n",
        "            clean_tokens.append(token.lemma_.lower())\n",
        "    return clean_tokens\n",
        "# Maps 'positive' to 1 and 'negative' to 0\n",
        "def convert_y(sentiments):\n",
        "    converted_list = []\n",
        "    for sentiment in sentiments:\n",
        "        converted_list.append(1 if sentiment == 'positive' else 0)\n",
        "    return np.array(converted_list)\n",
        "# Normalization function before neural network: normalizes all values between 0 and 1\n",
        "def normalize(arr):\n",
        "    max_val = np.max(arr)\n",
        "    min_val = np.min(arr)\n",
        "    # normalized_array = np.array([2 * ((val - min_val)/(max_val - min_val)) for val in arr])\n",
        "    normalized_array = 2 * ((arr - min_val)/(max_val - min_val)) - 1\n",
        "    return normalized_array\n",
        "\n",
        "#%%\n",
        "###### Loading Data ######\n",
        "!gdown --id 1EQCiEAMrmGZe4eGH990Lf5oyR7HK-z3x\n",
        "imdb = pd.read_csv('imdbDataset.csv')\n",
        "imdb = imdb.iloc[:data_size]\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "print(f\"Length : {len(imdb)}\")\n",
        "\n",
        "print(\"Data Loading Complete\")\n",
        "\n",
        "#%%\n",
        "##### Word -> Number Encoding ######\n",
        "##### Bag Of Words\n",
        "##### Hyperparams: max_features\n",
        "def bag_of_words(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading vectorizer...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/tfidf_vectors.pickle\", \"rb\"))\n",
        "    def create_model(X_data):\n",
        "        print(\"Creating vectorizer...\")\n",
        "        bow_transformer = CountVectorizer(analyzer=tokenize, max_features=2000).fit(X_data)\n",
        "        X_data = bow_transformer.transform(X_data)\n",
        "        os.mkdir(\"/content/preencoded_embeddings\") if not os.isdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(X_data, open(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\", \"wb\"))\n",
        "        return X_data\n",
        "\n",
        "    print(\"Starting Bag of Words Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/word2vec_reviews.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer.\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        print(\"Vectorizer Found.\")\n",
        "        X = load_model()\n",
        "    \n",
        "    y = convert_y(y)\n",
        "    print(\"Bag of Words Model Completed\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### TF-IDF Vectorization\n",
        "##### Hyperparams: max_features\n",
        "def tf_idf(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading vectorizer...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/tfidf_vectors.pickle\", \"rb\"))\n",
        "    def create_model(X_data):\n",
        "        print(\"Creating vectorizer...\")\n",
        "        tfidf_transformer = TfidfVectorizer(analyzer=tokenize, max_features=2000).fit(X_data)\n",
        "        X_data = tfidf_transformer.transform(X_data)\n",
        "        os.mkdir(\"/content/preencoded_embeddings\") if not os.isdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(X_data, open(\"/content/preencoded_embeddings/tfidf_vectors.pickle\", \"wb\"))\n",
        "        return X_data\n",
        "\n",
        "    print(\"Starting TF-IDF Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/word2vec_reviews.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer.\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        print(\"Vectorizer Found.\")\n",
        "        X = load_model()\n",
        "\n",
        "    y = convert_y(y)\n",
        "    print(\"TF-IDF Model Created\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### Pre-trained Word Embeddings (Word2Vec Twitter Model)\n",
        "##### Hyperparams: \n",
        "def word2vec(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading word vectors...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/word2vec_reviews.pickle\", \"rb\"))\n",
        "    def create_model(X):\n",
        "        vectorized_reviews = []\n",
        "        for index, review in enumerate(X):\n",
        "            avg_vector = np.zeros(shape=(300))\n",
        "            for token in tokenize(review):\n",
        "                avg_vector += nlp(token).vector\n",
        "            avg_vector = avg_vector / len(X)\n",
        "            vectorized_reviews.append(avg_vector)\n",
        "            print(f\"Review {index}: Done\")\n",
        "        os.mkdir(\"/content/preencoded_embeddings\") if not os.isdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(vectorized_reviews, open(\"/content/preencoded_embeddings/word2vec_reviews.pickle\", \"wb\"))\n",
        "        return vectorized_reviews\n",
        "\n",
        "    print(\"Starting Word2Vec Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/word2vec_reviews.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/word2vec_reviews.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        X = load_model()\n",
        "    y = convert_y(y)    \n",
        "    print(\"Word2Vec Model Created\")\n",
        "    return normalize(np.array(X)), y\n",
        "\n",
        "##### BERT Language Model\n",
        "\n",
        "##### Model Types ######\n",
        "##### Logistic Regression\n",
        "##### Hyperparams: train_test_split, regularization_type, C (inverse of regularization strength), \n",
        "def logistic_regression(X, y):\n",
        "    print(\"Starting creation of Logistic Regression Model\")\n",
        "    logistic_model = LogisticRegression()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "    y_pred = logistic_model.predict(X_test)\n",
        "    print(\"Logistic Regression Model created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### K-Nearest-Neighbours Classifier\n",
        "##### Hyperparams: n_neighbours, train_test_split\n",
        "def knn_classifier(X, y, number_neighbors):\n",
        "    print(\"Started creation of KNN Classifier\")\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=number_neighbors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    y_pred = knn_classifier.predict(X_test)\n",
        "    print(\"KNN Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Naive Bayes Classifier\n",
        "##### Hyperparams: train_test_split\n",
        "def naive_bayes(X, y):\n",
        "    print(\"Started creation of Naive Bayes Classifier\")\n",
        "    nb_model = MultinomialNB()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    print(\"Naive Bayes Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Neural Network\n",
        "##### Hyperparams: train_test_split, Architecture, Optimizer: Learning Rate, Epochs, Batch Size, Initial Weights, Initial Biases\n",
        "#####              Epochs, Loss Function, Regularization\n",
        "def neural_network(X, y, architecture_id):\n",
        "    print(\"Started creation of Neural Network\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n",
        "    if architecture_id == 1:\n",
        "        model = keras.Sequential([\n",
        "                keras.layers.Dense(300, activation=\"tanh\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(150, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=64)\n",
        "    elif architecture_id == 2:\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(2000, activation=\"relu\"),\n",
        "            keras.layers.Dense(200, activation=\"relu\"),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "    \n",
        "    model.summary()\n",
        "    y_pred = model.predict(X_test)\n",
        "    argmax_predictions = np.array([round(array[0]) for array in y_pred])\n",
        "    print(\"Neural Network Created\")\n",
        "    return y_test, argmax_predictions"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EQCiEAMrmGZe4eGH990Lf5oyR7HK-z3x\n",
            "To: /content/imdbDataset.csv\n",
            "66.2MB [00:00, 142MB/s] \n",
            "Length : 1500\n",
            "Data Loading Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkKWd_nuL2n7",
        "outputId": "9bc8b835-b894-4897-a3d8-f03efb54b054"
      },
      "source": [
        "##### Applying Models And Printing Accuracy #####\n",
        "\n",
        "X, y = word2vec(imdb['review'], imdb['sentiment'])\n",
        "y_test, y_pred = neural_network(X, y, 1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall: \", round(recall, 2))\n",
        "\n",
        "##### KNN Classifier Implementation\n",
        "# max_accuracy = -1\n",
        "# max_accuracy_neighbors = 0\n",
        "\n",
        "# for i in range(1, 10):\n",
        "#     print(f\"Number of neighbors: {i}\")\n",
        "#     y_test, y_pred = knn_classifier(X, y, i)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred)\n",
        "#     recall = recall_score(y_test, y_pred)\n",
        "\n",
        "#     print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall \", round(recall, 2))\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#     if (accuracy > max_accuracy):\n",
        "#         max_accuracy = accuracy\n",
        "#         max_accuracy_neighbors = i\n",
        "\n",
        "# print(f\"Maximum Accuracy obtained was {max_accuracy} with {max_accuracy_neighbors}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Word2Vec Model\n",
            "Loading word vectors...\n",
            "Word2Vec Model Created\n",
            "Started creation of Neural Network\n",
            "Epoch 1/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5450\n",
            "Epoch 2/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.6308\n",
            "Epoch 3/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.7217\n",
            "Epoch 4/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7033\n",
            "Epoch 5/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.7033\n",
            "Epoch 6/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.7458\n",
            "Epoch 7/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7608\n",
            "Epoch 8/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.7442\n",
            "Epoch 9/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.7158\n",
            "Epoch 10/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.5253 - accuracy: 0.7792\n",
            "Epoch 11/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7750\n",
            "Epoch 12/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.5120 - accuracy: 0.7625\n",
            "Epoch 13/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7550\n",
            "Epoch 14/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.8008\n",
            "Epoch 15/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7942\n",
            "Epoch 16/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7975\n",
            "Epoch 17/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.7725\n",
            "Epoch 18/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7683\n",
            "Epoch 19/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.8008\n",
            "Epoch 20/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7967\n",
            "Epoch 21/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.8008\n",
            "Epoch 22/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8025\n",
            "Epoch 23/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8042\n",
            "Epoch 24/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8183\n",
            "Epoch 25/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.8200\n",
            "Epoch 26/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8217\n",
            "Epoch 27/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4209 - accuracy: 0.8242\n",
            "Epoch 28/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4113 - accuracy: 0.8167\n",
            "Epoch 29/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.8008\n",
            "Epoch 30/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.8200\n",
            "Epoch 31/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8167\n",
            "Epoch 32/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8283\n",
            "Epoch 33/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8333\n",
            "Epoch 34/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8250\n",
            "Epoch 35/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8217\n",
            "Epoch 36/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8250\n",
            "Epoch 37/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8375\n",
            "Epoch 38/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3759 - accuracy: 0.8342\n",
            "Epoch 39/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8367\n",
            "Epoch 40/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8258\n",
            "Epoch 41/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8325\n",
            "Epoch 42/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8458\n",
            "Epoch 43/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8383\n",
            "Epoch 44/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8442\n",
            "Epoch 45/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8258\n",
            "Epoch 46/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.8383\n",
            "Epoch 47/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8258\n",
            "Epoch 48/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8375\n",
            "Epoch 49/50\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8525\n",
            "Epoch 50/50\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.8525\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 150)               45150     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 151       \n",
            "=================================================================\n",
            "Total params: 135,601\n",
            "Trainable params: 135,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Neural Network Created\n",
            "Accuracy:  0.83 , Precision:  0.82 , Recall:  0.85\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}