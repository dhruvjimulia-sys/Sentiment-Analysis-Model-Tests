{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB89UKxnL2nz"
      },
      "source": [
        "##### Global Variables ######\n",
        "create_new_vectors = False\n",
        "data_size = 1500"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8-2-YM7L2n3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022a337e-5ca9-40ab-80b6-d4d34176ebb7"
      },
      "source": [
        "###### Import Libraries ######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow import keras \n",
        "from scipy import sparse\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Imports complete\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051304 sha256=c76fd9f28686bcd0c1944fff9c91e76d23b920c24eb03677b8018ceb70e1e6d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-imvt3lxo/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Imports complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drraVU-VL2n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0513d3b2-ec00-401c-ccfe-984170823525"
      },
      "source": [
        "####### Data Preprocessing Functions #######\n",
        "def is_not_br(token):\n",
        "    return ((not ('<br' in token.text)) & (not ('/><br' in token.text)) & (not ('/>' in token.text)) & (token.text != 'br') & (not ('<' in token.text)))\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct) & is_not_br(token) & (not token.text.isdigit()):\n",
        "            clean_tokens.append(token.lemma_.lower())\n",
        "    return clean_tokens\n",
        "# Maps 'positive' to 1 and 'negative' to 0\n",
        "def convert_y(sentiments):\n",
        "    converted_list = []\n",
        "    for sentiment in sentiments:\n",
        "        converted_list.append(1 if sentiment == 'positive' else 0)\n",
        "    return np.array(converted_list)\n",
        "# Normalization function before neural network: normalizes all values between 0 and 1\n",
        "def normalize(arr):\n",
        "    max_val = np.max(arr)\n",
        "    min_val = np.min(arr)\n",
        "    # normalized_array = np.array([2 * ((val - min_val)/(max_val - min_val)) for val in arr])\n",
        "    normalized_array = 2 * ((arr - min_val)/(max_val - min_val)) - 1\n",
        "    return normalized_array\n",
        "\n",
        "#%%\n",
        "###### Loading Data ######\n",
        "!gdown --id 1EQCiEAMrmGZe4eGH990Lf5oyR7HK-z3x\n",
        "imdb = pd.read_csv('imdbDataset.csv')\n",
        "imdb = imdb.iloc[:data_size]\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "print(f\"Length : {len(imdb)}\")\n",
        "\n",
        "print(\"Data Loading Complete\")\n",
        "\n",
        "#%%\n",
        "##### Word -> Number Encoding ######\n",
        "##### Bag Of Words\n",
        "##### Hyperparams: max_features\n",
        "def bag_of_words(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading vectorizer...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\", \"rb\"))\n",
        "    def create_model(X_data):\n",
        "        print(\"Creating vectorizer...\")\n",
        "        bow_transformer = CountVectorizer(analyzer=tokenize, max_features=2000).fit(X_data)\n",
        "        X_data = bow_transformer.transform(X_data)\n",
        "        if not os.path.isdir(\"/content/preencoded_embeddings\"):\n",
        "          os.mkdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(X_data, open(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\", \"wb\"))\n",
        "        return X_data\n",
        "\n",
        "    print(\"Starting Bag of Words Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/bow_transformer_vectors.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer.\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        print(\"Vectorizer Found.\")\n",
        "        X = load_model()\n",
        "    \n",
        "    y = convert_y(y)\n",
        "    print(\"Bag of Words Model Completed\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### TF-IDF Vectorization\n",
        "##### Hyperparams: max_features\n",
        "def tf_idf(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading vectorizer...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/tfidf_vectors.pickle\", \"rb\"))\n",
        "    def create_model(X_data):\n",
        "        print(\"Creating vectorizer...\")\n",
        "        tfidf_transformer = TfidfVectorizer(analyzer=tokenize, max_features=2000).fit(X_data)\n",
        "        X_data = tfidf_transformer.transform(X_data)\n",
        "        if not os.path.isdir(\"/content/preencoded_embeddings\"):\n",
        "          os.mkdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(X_data, open(\"/content/preencoded_embeddings/tfidf_vectors.pickle\", \"wb\"))\n",
        "        return X_data\n",
        "\n",
        "    print(\"Starting TF-IDF Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/tfidf_vectors.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/tfidf_vectors.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer.\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        print(\"Vectorizer Found.\")\n",
        "        X = load_model()\n",
        "\n",
        "    y = convert_y(y)\n",
        "    print(\"TF-IDF Model Created\")\n",
        "    return sparse.lil_matrix(X).toarray(), y\n",
        "\n",
        "##### Pre-trained Word Embeddings (Word2Vec Twitter Model)\n",
        "##### Hyperparams: \n",
        "def word2vec(X, y):\n",
        "\n",
        "    def load_model():\n",
        "        print(\"Loading word vectors...\")\n",
        "        return pickle.load(open(\"/content/preencoded_embeddings/word2vec_reviews.pickle\", \"rb\"))\n",
        "    def create_model(X):\n",
        "        vectorized_reviews = []\n",
        "        for index, review in enumerate(X):\n",
        "            avg_vector = np.zeros(shape=(300))\n",
        "            for token in tokenize(review):\n",
        "                avg_vector += nlp(token).vector\n",
        "            avg_vector = avg_vector / len(X)\n",
        "            vectorized_reviews.append(avg_vector)\n",
        "            print(f\"Review {index}: Done\")\n",
        "        if not os.path.isdir(\"/content/preencoded_embeddings\"):\n",
        "          os.mkdir(\"/content/preencoded_embeddings\")\n",
        "        pickle.dump(vectorized_reviews, open(\"/content/preencoded_embeddings/word2vec_reviews.pickle\", \"wb\"))\n",
        "        return vectorized_reviews\n",
        "\n",
        "    print(\"Starting Word2Vec Model\")\n",
        "    if create_new_vectors:\n",
        "        if os.path.isfile(\"/content/preencoded_embeddings/word2vec_reviews.pickle\"):\n",
        "            user_input = input(\"Found vectorizer. Are you sure you still want to make a new vectorizer? (Y/N)\")\n",
        "            if user_input == \"N\":\n",
        "                X = load_model()\n",
        "            elif user_input == \"Y\":\n",
        "                os.remove(\"/content/preencoded_embeddings/word2vec_reviews.pickle\")\n",
        "                create_model(X)\n",
        "            else:\n",
        "                print(\"Input not in format specified\")\n",
        "        else:\n",
        "            print(\"Did not find vectorizer\")\n",
        "            X = create_model(X)\n",
        "    else:\n",
        "        X = load_model()\n",
        "    y = convert_y(y)    \n",
        "    print(\"Word2Vec Model Created\")\n",
        "    return normalize(np.array(X)), y\n",
        "\n",
        "##### BERT Language Model\n",
        "\n",
        "##### Model Types ######\n",
        "##### Logistic Regression\n",
        "##### Hyperparams: train_test_split, regularization_type, C (inverse of regularization strength), \n",
        "def logistic_regression(X, y):\n",
        "    print(\"Starting creation of Logistic Regression Model\")\n",
        "    logistic_model = LogisticRegression()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "    y_pred = logistic_model.predict(X_test)\n",
        "    print(\"Logistic Regression Model created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### K-Nearest-Neighbours Classifier\n",
        "##### Hyperparams: n_neighbours, train_test_split\n",
        "def knn_classifier(X, y, number_neighbors):\n",
        "    print(\"Started creation of KNN Classifier\")\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=number_neighbors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    y_pred = knn_classifier.predict(X_test)\n",
        "    print(\"KNN Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Naive Bayes Classifier\n",
        "##### Hyperparams: train_test_split\n",
        "def naive_bayes(X, y):\n",
        "    print(\"Started creation of Naive Bayes Classifier\")\n",
        "    nb_model = MultinomialNB()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    print(\"Naive Bayes Classifier created\")\n",
        "    return y_test, y_pred\n",
        "\n",
        "##### Neural Network\n",
        "##### Hyperparams: train_test_split, Architecture, Optimizer: Learning Rate, Epochs, Batch Size, Initial Weights, Initial Biases\n",
        "#####              Epochs, Loss Function, Regularization\n",
        "def neural_network(X, y, architecture_id):\n",
        "    print(\"Started creation of Neural Network\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n",
        "    if architecture_id == 1:\n",
        "        model = keras.Sequential([\n",
        "                keras.layers.Dense(300, activation=\"tanh\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(150, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomUniform(minval=-1, maxval=1), bias_initializer=keras.initializers.TruncatedNormal(mean=0, stddev=0.5)),\n",
        "                keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=64)\n",
        "    elif architecture_id == 2:\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(2000, activation=\"relu\"),\n",
        "            keras.layers.Dense(200, activation=\"relu\"),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "    \n",
        "    model.summary()\n",
        "    y_pred = model.predict(X_test)\n",
        "    argmax_predictions = np.array([round(array[0]) for array in y_pred])\n",
        "    print(\"Neural Network Created\")\n",
        "    return y_test, argmax_predictions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EQCiEAMrmGZe4eGH990Lf5oyR7HK-z3x\n",
            "To: /content/imdbDataset.csv\n",
            "66.2MB [00:00, 81.2MB/s]\n",
            "Length : 1500\n",
            "Data Loading Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKWd_nuL2n7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "3e3b0844-c8b4-49d8-f69e-a3a465ccdd10"
      },
      "source": [
        "##### Applying Models And Printing Accuracy #####\n",
        "\n",
        "X, y = bag_of_words(imdb['review'], imdb['sentiment'])\n",
        "y_test, y_pred = neural_network(X, y, 2)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall: \", round(recall, 2))\n",
        "\n",
        "##### KNN Classifier Implementation\n",
        "# max_accuracy = -1\n",
        "# max_accuracy_neighbors = 0\n",
        "\n",
        "# for i in range(1, 10):\n",
        "#     print(f\"Number of neighbors: {i}\")\n",
        "#     y_test, y_pred = knn_classifier(X, y, i)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred)\n",
        "#     recall = recall_score(y_test, y_pred)\n",
        "\n",
        "#     print(\"Accuracy: \", round(accuracy, 2), \", Precision: \", round(precision, 2), \", Recall \", round(recall, 2))\n",
        "#     print(\"\\n\")\n",
        "\n",
        "#     if (accuracy > max_accuracy):\n",
        "#         max_accuracy = accuracy\n",
        "#         max_accuracy_neighbors = i\n",
        "\n",
        "# print(f\"Maximum Accuracy obtained was {max_accuracy} with {max_accuracy_neighbors}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Bag of Words Model\n",
            "Vectorizer Found.\n",
            "Loading vectorizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6fd130bdf928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##### Applying Models And Printing Accuracy #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-aa129a4f9365>\u001b[0m in \u001b[0;36mbag_of_words\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vectorizer Found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-aa129a4f9365>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading vectorizer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/preencoded_embeddings/tfidf_vectors.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating vectorizer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/preencoded_embeddings/tfidf_vectors.pickle'"
          ]
        }
      ]
    }
  ]
}